# üß† InsureLLM RAG Challenge: The LLM Context Compression Approach

This document outlines an advanced, experimental architecture for the **InsureLLM Challenge**, designed as a companion to the *Relevance-Prioritized Truncation* method.

The core philosophy of this pipeline is to solve two persistent challenges simultaneously:

1. The strict **5,000-character context limit**.  
2. The **"Completeness" failure** (3.55 / 5) observed even in high-recall retrieval systems.

Instead of performing a "crude" truncation, this approach introduces a **second LLM call** as an intelligent *compression* step.  
It instructs the LLM to synthesize only the most relevant information from the retrieved chunks ‚Äî creating a single, dense context under the 5,000-character limit.

---

## üìä Data-Driven Diagnosis: Why This Was Needed

Our iterative evaluation revealed how retrieval improvements were not enough on their own.

1. **Baseline (Turn 13):** Naive RAG failed on complex queries.  
   - **MRR:** 0.7228  
   - **Completeness:** 3.56 / 5 (Red)

2. **Phase 1 (Turn 18):** Fixed indexing using `SemanticChunker`.[1, 2]  
   - **Holistic accuracy improved**, but **MRR crashed** to 0.6667 ‚Üí proving the retriever lacked precision.

3. **Phase 2 (Turn 22):** Introduced **Two-Stage Retrieval (Bi-Encoder + Cross-Encoder)**  
   - **MRR:** 0.6667 ‚Üí 0.9058  
   - **nDCG:** 0.6873 ‚Üí 0.9049  
   - Retrieval was nearly perfect ‚Äî but **Completeness** remained low at 3.55 / 5.

This indicated that even though we retrieved the right documents, the LLM‚Äôs context window and naive concatenation were *failing to synthesize complete answers*.

---

## üß† The Solution: LLM-Based Context Compression

This method solves both issues ‚Äî the 5,000-character constraint and the completeness gap ‚Äî through a **two-step LLM process**.

### üîÅ Step 1: The Compression Call

- Retrieve the top 3 documents using the cross-encoder re-ranker (`BAAI/bge-reranker-base`).
- Pass all 3 to a **compression LLM** via a `COMPRESSION_PROMPT_TEMPLATE`.
- The LLM synthesizes a single dense, query-specific text block under 5,000 characters.

### üí¨ Step 2: The Generation Call

- Feed the compressed text into the final `SYSTEM_PROMPT`.
- The LLM then generates the final answer based only on the **distilled, relevance-aware** context.

---

### ‚öñÔ∏è Pros and Cons

**‚úÖ Pros:**
- Smarter than truncation ‚Äî synthesizes across multiple documents.  
- Greatly improves ‚ÄúCompleteness‚Äù by combining top-3 relevant chunks.  

**‚ö†Ô∏è Cons:**
- **Slower and costlier** (two LLM calls per query).  
- **Risk of LLM hallucination** or over-compression if summarization fails.[3]

---

## üèóÔ∏è Final Code Architecture

### `ingest.py` (Final Version)

This component uses `SemanticChunker` to create coherent, semantically consistent chunks (same as in the truncation approach).

```python
import os
import glob
from pathlib import Path
from langchain_community.document_loaders import DirectoryLoader, TextLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv

MODEL = "gpt-4.1-nano"

DB_NAME = str(Path(__file__).parent.parent / "vector_db")
KNOWLEDGE_BASE = str(Path(__file__).parent.parent / "knowledge-base")

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
load_dotenv(override=True)

def fetch_documents():
    folders = glob.glob(str(Path(KNOWLEDGE_BASE) / "*"))
    documents = []
    for folder in folders:
        doc_type = os.path.basename(folder)
        loader = DirectoryLoader(
            folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}
        )
        folder_docs = loader.load()
        for doc in folder_docs:
            doc.metadata["doc_type"] = doc_type
        documents.extend(folder_docs)
    return documents

def create_chunks(documents):
    text_splitter = SemanticChunker(embeddings)
    chunks = text_splitter.split_documents(documents)
    return chunks

def create_embeddings(chunks):
    if os.path.exists(DB_NAME):
        Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()
    vectorstore = Chroma.from_documents(
        documents=chunks, embedding=embeddings, persist_directory=DB_NAME
    )
    return vectorstore

if __name__ == "__main__":
    documents = fetch_documents()
    chunks = create_chunks(documents)
    create_embeddings(chunks)
    print("Ingestion complete")
```

---

### `answer.py` (Final Version ‚Äì LLM Compression)

Implements **Two-Stage Retrieval** with **LLM Context Compression**.

```python
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, convert_to_messages
from typing import List, Optional, Tuple

# New imports for compression
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate 

# Classic and community imports
from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors.cross_encoder_rerank import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from dotenv import load_dotenv
load_dotenv(override=True)

MODEL = "gpt-4.1-nano"
DB_NAME = str(Path(__file__).parent.parent / "vector_db")

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
MAX_CONTEXT_LENGTH = 5000

SYSTEM_PROMPT = """
You are a knowledgeable assistant representing InsureLLM.
Use ONLY the provided context to answer the user's question.
If the context is insufficient, say you cannot answer based on the documents.

Context:
{context}
"""

# --- Compression Prompt Template ---
COMPRESSION_PROMPT_TEMPLATE = """You are a highly intelligent text analysis assistant.
You will be given a user's query and several context chunks retrieved by a search system.
Analyze them and extract ONLY the information relevant to answering the query.

Synthesize this information into a single, coherent, dense block of text.
The output MUST be under 5000 characters.

If no relevant information exists, respond with exactly: "NO_RELEVANT_INFORMATION".

Context Chunks:
{context_chunks}

User Query: {question}

Relevant Information (under 5000 chars):"""

# --- Two-Stage Retrieval Setup ---
vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 50})
model_name = "BAAI/bge-reranker-base"
cross_encoder = HuggingFaceCrossEncoder(model_name=model_name, model_kwargs={'device': 'cpu'})
compressor = CrossEncoderReranker(model=cross_encoder, top_n=3)
retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)

# --- LLMs ---
llm = ChatOpenAI(temperature=0, model_name=MODEL)
compression_prompt = ChatPromptTemplate.from_template(COMPRESSION_PROMPT_TEMPLATE)
compression_chain = compression_prompt | llm | StrOutputParser()

def fetch_context(question: str) -> list:
    print(f"\n--- FETCHING CONTEXT FOR: '{question}' ---")
    docs = retriever.invoke(question)
    print(f"Retrieved {len(docs)} re-ranked documents.")
    return docs

def answer_question(question: str, history: Optional[List] = None) -> Tuple[str, List]:
    if history is None:
        history = []

    # Step 1: Retrieve
    docs = fetch_context(question)
    if not docs:
        return "No relevant documents found.", []

    # Step 2: LLM Compression
    print(f"\n--- COMPRESSING CONTEXT ---")
    context_for_compression = "\n\n---\n\n".join(getattr(doc, "page_content", "") for doc in docs)
    compressed_context = compression_chain.invoke({"context_chunks": context_for_compression, "question": question})

    if "NO_RELEVANT_INFORMATION" in compressed_context or not compressed_context.strip():
        return "No relevant information found after compression.", docs

    final_context = compressed_context[:MAX_CONTEXT_LENGTH]
    print(f"Final compressed context length: {len(final_context)} characters.")

    # Step 3: Final Answer Generation
    system_prompt = SYSTEM_PROMPT.format(context=final_context)
    messages = []
    messages.extend(convert_to_messages(history))
    messages.append(HumanMessage(content=question))
    response = llm.invoke(messages)
    return response.content, docs
```

---

## üñºÔ∏è Visual Results

*(Add your result images below ‚Äî replace these placeholders with actual paths once available.)*

### üìà Comparison of Compression vs. Truncation

![Compression Results Placeholder](images/compression_results.png)

### üß© Context Compression Flow Diagram

![Compression Flow Placeholder](images/compression_flow.png)

---

## üß∞ Installation

Please install the required packages before running the pipeline:

```bash
uv pip install langchain-experimental langchain-classic langchain-community
```

---

## üìö References

1. LangChain Experimental ‚Äì [SemanticChunker Documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic_chunker)
2. HuggingFace ‚Äì [MiniLM Embeddings](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
3. BAAI ‚Äì [bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
4. Microsoft Research ‚Äì *Dual Encoder vs. Cross Encoder in Dense Retrieval*

---

**Author:** Your Name  
**Project:** InsureLLM RAG Challenge  
**Result:** üß© Experimental ‚ÄúLLM Compression‚Äù pipeline ‚Äî designed to improve *Completeness* while maintaining a 0.90+ MRR.
